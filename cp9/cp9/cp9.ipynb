{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Pr\u00e1ctica #9 (Compilaci\u00f3n)\n",
    "\n",
    "En esta clase estaremos implementando un **generador de lexer**. Nos apoyaremos en el int\u00e9rprete de expresiones regulares que implementamos en la clase anterior.\n",
    "\n",
    "Por cuesti\u00f3n de comodidad, en esta clase usaremos una versi\u00f3n de aut\u00f3mata basada en referencias entre estados.\n",
    "La clase `State`, que usaremos para modelar los estados, se encuentra en `cmp.automata`. Pasemos a importala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.automata import State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El aut\u00f3mata resulta de la interconexi\u00f3n de estados. Cualquier estado puede usarse como ra\u00edz del aut\u00f3mata, pero en funci\u00f3n del estado que se escoja ser\u00e1 el lenguaje reconocido. Un estado se contruye a partir de un objeto, que se usar\u00e1 para representarlo. Este puede ser un entero, o string, pero tambi\u00e9n pueden ser otros estados a su vez. Adem\u00e1s, es necesario especificar si el estado es final o no. Por defecto se asume que **no** es final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = State(0)\n",
    "b = State(1, True)\n",
    "a.add_transition('a', a)\n",
    "a.add_transition('b', b)\n",
    "b.add_transition('a', a)\n",
    "b.add_transition('b', b)\n",
    "\n",
    "display(a)\n",
    "\n",
    "print('----------- Estado 0 -------------')\n",
    "print('Identificador:', a.state)\n",
    "print('Es final:', a.final)\n",
    "print('Transiciones:', a.transitions)\n",
    "print('Epsilon transiciones:', a.epsilon_transitions)\n",
    "\n",
    "display(b)\n",
    "\n",
    "print('----------- Estado 1 -------------')\n",
    "print('Identificador:', b.state)\n",
    "print('Es final:', b.final)\n",
    "print('Transiciones:', b.transitions)\n",
    "print('Epsilon transiciones:', b.epsilon_transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las **transiciones** entre estados se a\u00f1aden a trav\u00e9s del m\u00e9todo `<origin>.add_transition(symbol, end)`. En el caso de las **epsilon transiciones**, se usa `<origin>.add_epsilon_transition(end)`. Las transiciones entre estados son potencialmente no deterministas. Depender\u00e1 del aut\u00f3mata construido si se comporta de dicha forma o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = State(2)\n",
    "c.add_epsilon_transition(a)\n",
    "c.add_epsilon_transition(b)\n",
    "\n",
    "display(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se provee un mecanismo de conversi\u00f3n de aut\u00f3mata no determinista a determinista. N\u00f3tese que la transformaci\u00f3n es solo sobre la estructura del aut\u00f3mata subyacente (se sigue usando la misma clase `State`). Los nodos del aut\u00f3mata original pasan ha formar parte del identificador de los nuevos estados: al consultar el campo `state`, se obtiene una tupla con los posibles estados en los que podr\u00eda estar el aut\u00f3mata original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(c.to_deterministic())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase `State` brinda el m\u00e9todo `from_nfa` por compatibilidad con la API de aut\u00f3matas que estuvimos usando hasta la clase anterior. Este m\u00e9todo construye una instancia de `State` a partir de una instancia de `NFA` (o por tanto de `DFA`). Se puede incluir un segundo argumento al llamado del m\u00e9todo `from_nfa` si se desea obtener, adem\u00e1s del estado inicial del aut\u00f3mata resultante, el resto de los estados que fueron creados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.tools.automata import DFA\n",
    "\n",
    "automaton = DFA(states=3, finals=[2], transitions={\n",
    "    (0, 'a'): 0,\n",
    "    (0, 'b'): 1,\n",
    "    (1, 'a'): 2,\n",
    "    (1, 'b'): 1,\n",
    "    (2, 'a'): 0,\n",
    "    (2, 'b'): 1,\n",
    "})\n",
    "print('Original (DFA):')\n",
    "display(automaton)\n",
    "\n",
    "automaton = State.from_nfa(automaton)\n",
    "print('Copia (State):')\n",
    "display(automaton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adem\u00e1s, se provee un mecanismo de reconocimiento de cadenas (independientemente de si el aut\u00f3mata subyacente es determinista o no). Este es accesible a trav\u00e9s del m\u00e9todo `<state>.recognize(string)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert automaton.recognize('ba')\n",
    "assert automaton.recognize('aababbaba')\n",
    "\n",
    "assert not automaton.recognize('')\n",
    "assert not automaton.recognize('aabaa')\n",
    "assert not automaton.recognize('aababb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generador de Lexer\n",
    "\n",
    "Como estudiamos en conferencia, el generador de lexer se basa en un conjunto de expresiones regulares. Cada una de ellas est\u00e1 asociada a un tipo de token. El lexer termina siendo un aut\u00f3mata finito determinista con ciertas peculiaridades:\n",
    "- Resulta de transformar el aut\u00f3mata uni\u00f3n entre todas las expresiones regulares del lenguaje, y convertirlo a determinista.\n",
    "- Cada estado final almacena los tipos de tokens que se reconocen al alcanzarlo. Se establece una prioridad entre ellos para poder desambiaguar.\n",
    "- Para tokenizar, la cadena de entrada viaja repetidas veces por el aut\u00f3mata.\n",
    "    - Cada vez, se comienza por el estado inicial, pero se contin\u00faa a partir de la \u00faltima secci\u00f3n de la cadena que fue reconocida.\n",
    "    - Cuando el aut\u00f3mata se \"traba\" durante el reconocimiento de una cadena, se reporta la ocurrencia de un token. Su lexema est\u00e1 formado por la concatenaci\u00f3n de los s\u00edmbolos que fueron consumidos desde el inicio y hasta pasar por el \u00faltimo estado final antes de trabarse. Su tipo de token es el de mayor relevancia entre los anotados en el estado final.\n",
    "    - Al finalizar de consumir toda la cadena, se reporta el token de fin de cadena.\n",
    "\n",
    "### Expresiones regulares\n",
    "\n",
    "El engine de expresiones regulares que implementamos en la clase anterior est\u00e1 disponible en `cmp.tool.regex`. La clase `Regex` se instancia a partir de un string que representa la expresi\u00f3n regular. Cada instancia posee un campo `automaton` que permite acceder al `DFA` que reconoce el lenguaje denotado por dicha expresi\u00f3n regular. Las instancias de `Regex` son invocables, recibiendo como \u00fanico par\u00e1mentro un string, y devuelve si el string pertenece o no al lenguaje denotado por la expresi\u00f3n regular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.tools.regex import Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexer\n",
    "\n",
    "Implementemos el generador de lexer. El lexer se construir\u00e1 a partir de la tabla de expresiones regulares (una lista de tuplas con la forma: `(<token_type>, <regex_str>)`). Esta tabla se recibe como el par\u00e1metro `table` en el contructor de la clase `Lexer`. La prioridad/relevancia de cada tipo de token est\u00e1 marcada por el \u00edndice que ocupa en la tabla. Los tipos de tokens cuyas expresiones regulares est\u00e9n registradas m\u00e1s cerca del inicio de la tabla tiene m\u00e1s prioridad.\n",
    "- **_build_regexs:** devuelve un lista con los aut\u00f3matas (instancias de `State`) de cada expresi\u00f3n regular. Los estados finales de los respectivos aut\u00f3matas deben marcarse (campo `tag`) con la prioridad y tipo de token seg\u00fan la expresi\u00f3n regular que lo origin\u00f3.\n",
    "- **_build_automaton:** devuelve la versi\u00f3n determinista del aut\u00f3mata que reconoce los tokens del lenguaje.\n",
    "- **_walk:** Devuelve el \u00faltimo estado final visitado, y lexema consumido, durante el reconocimiento del string que se recibe como entrada.\n",
    "- **_tokenize:** Devuelve tuplas de la forma `(lex, token_type)` que resultan de tokenizar la entrada. Debe manejar el caso en que la entrada no puede ser tokenizada completamente (se detecta cuando en una iteraci\u00f3n la cadena no avanz\u00f3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.utils import Token\n",
    "\n",
    "class Lexer:\n",
    "    def __init__(self, table, eof):\n",
    "        self.eof = eof\n",
    "        self.regexs = self._build_regexs(table)\n",
    "        self.automaton = self._build_automaton()\n",
    "    \n",
    "    def _build_regexs(self, table):\n",
    "        regexs = []\n",
    "        for n, (token_type, regex) in enumerate(table):\n",
    "            # Your code here!!!\n",
    "            # - Remember to tag the final states with the token_type and priority.\n",
    "            # - <State>.tag might be useful for that purpose ;-)\n",
    "            pass\n",
    "        return regexs\n",
    "    \n",
    "    def _build_automaton(self):\n",
    "        start = State('start')\n",
    "        # Your code here!!!\n",
    "        return start.to_deterministic()\n",
    "    \n",
    "        \n",
    "    def _walk(self, string):\n",
    "        state = self.automaton\n",
    "        final = state if state.final else None\n",
    "        final_lex = lex = ''\n",
    "        \n",
    "        for symbol in string:\n",
    "            # Your code here!!!\n",
    "            pass\n",
    "            \n",
    "        return final, final_lex\n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        # Your code here!!!\n",
    "        \n",
    "        yield '$', self.eof\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        return [ Token(lex, ttype) for lex, ttype in self._tokenize(text) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construyamos un lexer con algunas expresiones regulares solo para comprobar la validez de la implementaci\u00f3n. Usaremos `str` para marcar los tipos de tokens, pero recuerde que realmente se usan los s\u00edmbolos de la gram\u00e1tica ya que son los s\u00edmbolos con los que trabaja el parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_digits = '|'.join(str(n) for n in range(1,10))\n",
    "letters = '|'.join(chr(n) for n in range(ord('a'),ord('z')+1))\n",
    "\n",
    "print('Non-zero digits:', nonzero_digits)\n",
    "print('Letters:', letters)\n",
    "\n",
    "lexer = Lexer([\n",
    "    ('num', f'({nonzero_digits})(0|{nonzero_digits})*'),\n",
    "    ('for' , 'for'),\n",
    "    ('foreach' , 'foreach'),\n",
    "    ('space', '  *'),\n",
    "    ('id', f'({letters})({letters}|0|{nonzero_digits})*')\n",
    "], 'eof')\n",
    "\n",
    "text = '5465 for 45foreach fore'\n",
    "print(f'\\n>>> Tokenizando: \"{text}\"')\n",
    "tokens = lexer(text)\n",
    "print(tokens)\n",
    "assert [t.token_type for t in tokens] == ['num', 'space', 'for', 'space', 'num', 'foreach', 'space', 'id', 'eof']\n",
    "assert [t.lex for t in tokens] == ['5465', ' ', 'for', ' ', '45', 'foreach', ' ', 'fore', '$']\n",
    "\n",
    "text = '4forense forforeach for4foreach foreach 4for'\n",
    "print(f'\\n>>> Tokenizando: \"{text}\"')\n",
    "tokens = lexer(text)\n",
    "print(tokens)\n",
    "assert [t.token_type for t in tokens] == ['num', 'id', 'space', 'id', 'space', 'id', 'space', 'foreach', 'space', 'num', 'for', 'eof']\n",
    "assert [t.lex for t in tokens] == ['4', 'forense', ' ', 'forforeach', ' ', 'for4foreach', ' ', 'foreach', ' ', '4', 'for', '$']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propuestas\n",
    "\n",
    "- Genere un lexer para el lenguaje de expresiones aritm\u00e9ticas que estuvimos trabajando a inicios del curso.\n",
    "- Implemente un algoritmo para eliminar los estados muertos de un aut\u00f3mata. Recordemos que esto es conveniente puesto que de esta forma el aut\u00f3mata del lexer detecta los tokens m\u00e1s de prisa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}