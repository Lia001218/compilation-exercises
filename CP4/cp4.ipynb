{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Práctica #4 (Compilación)\n",
    "\n",
    "En esta clase estaremos implementando un mecanismo genérico de **evaluación** de cadenas a partir de la especificación de **atributos** y **reglas** en la gramática. Diseñaremos concretamente las reglas de evaluación para la gramática del subconjunto de `HULK` con que hemos trabajado desde clases anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gramáticas Atributadas\n",
    "\n",
    "Recordemos que una **gramática atributada** es una tupla $<G,A,R>$, donde:\n",
    "\n",
    "* $G = <S,P,N,T>$ es una gramática libre del contexto,\n",
    "* $A$ es un conjunto de atributos de la forma $X \\cdot a$\n",
    "  donde $X \\in N \\cup T$ y $a$ es un identificador único entre todos los atributos del mismo símbolo, y\n",
    "* $R$ es un conjunto de reglas de la forma $<p_i, r_i>$ donde $p_i \\in P$ es una producción $X \\to Y_1, \\ldots, Y_n$, y $r_i$ es una regla de la forma:\n",
    "    1. $X \\cdot a = f(Y_1 \\cdot a_1, \\ldots, Y_n \\cdot a_n)$, o\n",
    "    2. $Y_i \\cdot a = f(X \\cdot a_0, Y_1 \\cdot a_1, \\ldots, Y_n \\cdot a_n)$.\n",
    "\n",
    "Los atributos se dividen en dos conjuntos disjuntos: _atributos heredados_ y _atributos sintetizados_. En el caso (1) decimos que `a` es un _atributo sintetizado_, y en el caso (2), un _atributo heredado_.\n",
    "\n",
    "Según esta distinción, estudiamos en conferencia condiciones suficientes para que una gramática fuera evaluable:\n",
    "\n",
    "- Una gramática atributada es **s-atributada** si y solo si, para toda regla $r_i$ asociada a una producción $X \\to Y_1, \\ldots, Y_n$, se cumple que $r_i$ es de la forma $X \\cdot a = f(Y_1 \\cdot a_1, \\ldots, Y_n \\cdot a_n)$.\n",
    "\n",
    "- Una gramática atributada es **l-atributada** si y solo si toda regla $r_i$ asociada a una producción $X \\to Y_1, \\ldots, Y_n$ es de una de las siguientes formas:\n",
    "    - $X \\cdot a = f(Y_1 \\cdot a_1, \\ldots, Y_n \\cdot a_n)$, ó\n",
    "    - $Y_i \\cdot a_i = f(X \\cdot a, Y_1 \\cdot a_1, \\ldots, Y_{i-1} \\cdot a_{i-1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Especificación en _Python_\n",
    "\n",
    "Continuaremos trabajando con la _API_ para gramáticas presentada en la clase anterior. Esta vez, la definición de símbolo, oración, producción, gramática, etc. se encuentra en el módulo `cmp` que se distribuye junto a este _notebook_.\n",
    "\n",
    "Procedamos a importar las clases y métodos que nos interesan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.pycompiler import Symbol, NonTerminal, Terminal, EOF, Sentence, Epsilon, Production, Grammar\n",
    "from cmp.utils import pprint, inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la _API_ de gramáticas se añade una nueva clase: `AttributeProduction`. Con esta clase modelaremos las producciones de las gramáticas atributadas. Cada una de estas producciones se compone por:\n",
    "- Un no terminal como cabecera. Accesible a través del campo `Left`.\n",
    "- Una oración como cuerpo. Accesible a través del campo `Right`.\n",
    "- Un conjunto de reglas para evaluar los atributos. Accesible a través del campo `atributes`.\n",
    "\n",
    "Las producciones no deben ser instanciadas directamente con la aplicación de su constructor. En su lugar, se presentan las siguientes facilidades para formar producciones a partir de una instancia `G` de `Grammar` y un grupo de terminales y no terminales:\n",
    "- Para definir una producción de la forma $B_0 \\to B_1 B_2 ... B_n$ que:\n",
    "    - Asocia a $B_0$ una regla $\\lambda_0$ para sintetizar sus atributos, y\n",
    "    - Asocia a $B_1 \\dots B_n$ las reglas $\\lambda_1 \\dots \\lambda_n$ que hereden sus atributos respectivamentes.\n",
    "    \n",
    "    ```python\n",
    "    B0 %= B1 + B2 + ... + Bn, lambda0, lambda1, lambda2, ..., lambdaN\n",
    "    ```\n",
    "    \n",
    "> Donde `lambda0`, `lambda1`, ..., `lambdaN` son funciones que reciben 2 parámetros.\n",
    "> 1. Como primer parámetro los atributos heredados que se han computado para cada instancia de símbolo en la producción, durante la aplicación de esa instancia de producción específicamente. Los valores se acceden desde una lista de `n + 1` elementos. Los valores se ordenan según aparecen los símbolos en la producción, comenzando por la cabecera. Nos referiremos a esta colección como `inherited`.\n",
    "> 2. Como segundo parámetro los atributos sintetizados que se han computado para cada instancia de símbolo en la producción, durante la aplicación de esa instancia de producción específicamente. Sigue la misma estructura que el primer parámetro. Nos referiremos a esta colección como `synteticed`.\n",
    ">\n",
    "> La función `lambda0` sintetiza los atributos de la cabecera. La evaluación de dicha función produce el valor de `synteticed[0]`. El resto de los atributos sintetizados de los símbolos de la producción se calcula de la siguiente forma:\n",
    "> - En caso de que el símbolo sea un terminal, evalúa como su lexema.\n",
    "> - En caso de que el símbolo sea un no terminal, se obtiene de evaluar la función `lambda0` en la instancia de producción correspondiente.\n",
    ">\n",
    "> La función `lambda_i`, con `i` entre 1 y `n`, computa los atributos heredados de la i-ésima ocurrencia de símbolo en la producción. La evaluación de dicha función produce el valor de `inherited[i]`. El valor de `inherited[0]` se obtiene como el atributo que heredó la instancia concreta del símbolo en la cabecera antes de comenzar a aplicar la producción.\n",
    "\n",
    "- En caso de que no se vaya a sociar una regla a un símbolo se incluirá un `None`.\n",
    "    ```python\n",
    "       E %= T + X   ,  lambda h,s: s[2]  ,    None    ,   lambda h,s: s[1]\n",
    "    # ___________     ________________     ________      ________________\n",
    "    # producción  |    regla para E    |  sin regla  |     regla para X \n",
    "    ```\n",
    "    > `[0]:` **`lambda h,s: s[2]`** al ser `lambda0` sintetiza el valor de `E`. Lo hace en función del valor que sintetiza `X` (accesible desde `s[2]`).  \n",
    "    > `[1]:` **`None`** al ser `lambda1` indica que no se incluye regla para heredar valor a `T`.  \n",
    "    > `[2]:` **`lambda h,s: s[1]`** al ser `lambda2` hereda un valor a `X`. Lo hace en función del valor que sintetiza `T` (accesible desde `s[1]`).\n",
    "\n",
    "- No se deben definir múltiples producciones de la misma cabecera en una única sentencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributeProduction(Production):\n",
    "\n",
    "    def __init__(self, nonTerminal, sentence, attributes):\n",
    "        if not isinstance(sentence, Sentence) and isinstance(sentence, Symbol):\n",
    "            sentence = Sentence(sentence)\n",
    "        super(AttributeProduction, self).__init__(nonTerminal, sentence)\n",
    "\n",
    "        self.attributes = attributes\n",
    "\n",
    "    def __str__(self):\n",
    "        return '%s := %s' % (self.Left, self.Right)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '%s -> %s' % (self.Left, self.Right)\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield self.Left\n",
    "        yield self.Right\n",
    "\n",
    "\n",
    "    @property\n",
    "    def IsEpsilon(self):\n",
    "        return self.Right.IsEpsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gramática de HULK\n",
    "\n",
    "Completemos entonces la siguiente especificación de la gramática para `HULK` añadiendo las reglas necesarias.\n",
    "\n",
    "`E` $\\rightarrow$ `T X`  \n",
    "`X` $\\rightarrow$ `+ T X | - T X | epsilon`  \n",
    "`T` $\\rightarrow$ `F Y`  \n",
    "`Y` $\\rightarrow$ `* F Y | / F Y | epsilon`  \n",
    "`F` $\\rightarrow$ `( E ) | num`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Terminals:\n",
      "\tE, T, F, X, Y\n",
      "Terminals:\n",
      "\t+, -, *, /, (, ), num\n",
      "Productions:\n",
      "\t[E -> T X, X -> + T X, X -> - T X, X -> e, T -> F Y, Y -> * F Y, Y -> / F Y, Y -> e, F -> num, F -> ( E )]\n"
     ]
    }
   ],
   "source": [
    "G = Grammar()\n",
    "E = G.NonTerminal('E', True)\n",
    "T, F, X, Y = G.NonTerminals('T F X Y')\n",
    "plus, minus, star, div, opar, cpar, num = G.Terminals('+ - * / ( ) num')\n",
    "\n",
    "############################ BEGIN PRODUCTIONS ############################\n",
    "# ======================================================================= #\n",
    "#                                                                         #\n",
    "# ========================== { E --> T X } ============================== #\n",
    "#                                                                         #\n",
    "E %= T + X, lambda h,s: s[2], None, lambda h,s: s[1]                    #\n",
    "#                                                                         #\n",
    "# =================== { X --> + T X | - T X | epsilon } ================= #\n",
    "#                                                                         #\n",
    "X %= plus + T + X, lambda h,s: s[3], None, None , lambda h,s: h[0] + s[2]       #                               \n",
    "X %= minus + T + X, lambda h,s: s[3], None, None, lambda h,s: h[0] - s[2]       #\n",
    "X %= G.Epsilon, lambda h,s: h[0]\n",
    "#                                                                         #\n",
    "# ============================ { T --> F Y } ============================ #\n",
    "#                                                                         #\n",
    "T %= F + Y, lambda h,s: s[2], None, lambda h,s: s[1]                    # \n",
    "#                                                                         #\n",
    "# ==================== { Y --> * F Y | / F Y | epsilon } ================ #\n",
    "#                                                                         #\n",
    "Y %= star + F + Y, lambda h,s: s[3], None, None, lambda h,s: h[0] * s[2]       #\n",
    "Y %= div + F + Y, lambda h,s: s[3], None, None, lambda h,s: h[0] / s[2]       #\n",
    "Y %= G.Epsilon, lambda h,s: h[0]\n",
    "#                                                                         #\n",
    "# ======================= { F --> num | ( E ) } ========================= #\n",
    "F %= num, lambda h,s: float(s[1]), None\n",
    "F %= opar + E + cpar , lambda h,s: s[2] ,None , None , None \n",
    "#                                                                         #\n",
    "# ======================================================================= #\n",
    "############################# END PRODUCTIONS #############################\n",
    "\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la clase pasada implementamos los algoritmos para calcular los conjuntos `first` y `follow`. Esta vez utilizaremos dichos conjuntos ya precomputados para nuestro subconjunto de `HULK`. Pasemos a importarlos desde el módulo `utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.languages import BasicHulk\n",
    "\n",
    "hulk = BasicHulk(G)\n",
    "firsts, follows = hulk.firsts, hulk.follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma similar procederemos con los métodos `build_parsing_table` y `metodo_predictivo_no_recursivo` que devuelven la tabla _LL(1)_ y el parser _LL(1)_ respectivamente. Pasemos a importarlos desde el módulo `tools`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.tools.parsing import build_parsing_table\n",
    "from cmp.tools.parsing import deprecated_metodo_predictivo_no_recursivo as metodo_predictivo_no_recursivo\n",
    "\n",
    "# Testing table\n",
    "M = build_parsing_table(G, firsts, follows)\n",
    "assert M == hulk.table\n",
    "\n",
    "# Testing parser\n",
    "parser = metodo_predictivo_no_recursivo(G, M)\n",
    "left_parse = parser([num, star, num, star, num, plus, num, star, num, plus, num, plus, num, G.EOF])\n",
    "assert left_parse == [ \n",
    "   Production(E, Sentence(T, X)),\n",
    "   Production(T, Sentence(F, Y)),\n",
    "   Production(F, Sentence(num)),\n",
    "   Production(Y, Sentence(star, F, Y)),\n",
    "   Production(F, Sentence(num)),\n",
    "   Production(Y, Sentence(star, F, Y)),\n",
    "   Production(F, Sentence(num)),\n",
    "   Production(Y, G.Epsilon),\n",
    "   Production(X, Sentence(plus, T, X)),\n",
    "   Production(T, Sentence(F, Y)),\n",
    "   Production(F, Sentence(num)),\n",
    "   Production(Y, Sentence(star, F, Y)),\n",
    "   Production(F, Sentence(num)),\n",
    "   Production(Y, G.Epsilon),\n",
    "   Production(X, Sentence(plus, T, X)),\n",
    "   Production(T, Sentence(F, Y)),\n",
    "   Production(F, Sentence(num)),\n",
    "   Production(Y, G.Epsilon),\n",
    "   Production(X, Sentence(plus, T, X)),\n",
    "   Production(T, Sentence(F, Y)),\n",
    "   Production(F, Sentence(num)),\n",
    "   Production(Y, G.Epsilon),\n",
    "   Production(X, G.Epsilon),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación\n",
    "\n",
    "En la clase anterior asumimos que la cadena de entrada que queremos parsear es una lista de símbolos terminales. Aún así, notemos que en realidad la entrada no está compuesta solamente por estos símbolos. El parser trabaja con una secuencia de _tokens_, que como ya sabemos se componen de un _lexema_ y un _tipo_. Los símbolos terminales son justamente los tipos de los tokens y, por tanto, son los valores relevantes al parsear. Sin embargo, nuestro problema no termina al parsear sino que debemos ser capaces de evaluar, en el lenguaje actual, la expresión de `HULK` que se dió como entrada. Para ello, el lexema de los tokens juega un papel esencial ya que son estos los que capturan las particularidades de los valores de entrada. Por ejemplo, en el caso de `HULK`, para saber qué dos números se están operando es necesario considerar los lexemas.\n",
    "\n",
    "A continuación se implementa la clase `Token` usada para modelar los tokens. Se puede acceder al lexema y tipo de cada token a través de los campos `lex` y `token_type` respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "    \"\"\"\n",
    "    Basic token class. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lex : str\n",
    "        Token's lexeme.\n",
    "    token_type : Enum\n",
    "        Token's type.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lex, token_type):\n",
    "        self.lex = lex\n",
    "        self.token_type = token_type\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'{self.token_type}: {self.lex}'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifiquemos el generador de parsers para que acceda el tipo de token a través de la propiedad `token_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "deprecated_metodo_predictivo_no_recursivo = metodo_predictivo_no_recursivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OJO: No ejecute la celda anterior ($\\uparrow$) una vez ejecutadas las celdas que siguen a continuación ($\\downarrow$)**\n",
    "\n",
    "Redefiniremos la implementación del generador de parsers hacia una que *decore* la salida del actual. Esta nueva implementación simplemente extraerá de los tokens de entrada los respectivos tipos (`token_type`), y procederá de la misma forma que ya estaba implementada. Claramente, los hacemos de esta forma para reutilizar la versión que ya teníamos implementada, pero podríamos reescribir la implementación original para que al acceder al símbolo puntado por el cabezal (`a = w[cursor]`) accediera a su tipo a través del campo `token_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metodo_predictivo_no_recursivo(G, M):\n",
    "    parser = deprecated_metodo_predictivo_no_recursivo(G, M)\n",
    "    def updated(tokens):\n",
    "        return parser([t.token_type for t in tokens])\n",
    "    return updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rápidamente podemos comprobar la efectividad del cambio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[E -> T X,\n",
       " T -> F Y,\n",
       " F -> num,\n",
       " Y -> e,\n",
       " X -> + T X,\n",
       " T -> F Y,\n",
       " F -> num,\n",
       " Y -> e,\n",
       " X -> e]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib2to3.pgen2 import token\n",
    "\n",
    "\n",
    "text = '5.9 + 4'\n",
    "tokens = [ Token('5.9', num), Token('+', plus), Token('4', num), Token('$', G.EOF) ]\n",
    "parser = metodo_predictivo_no_recursivo(G, M)\n",
    "left_parse = parser(tokens)\n",
    "left_parse\n",
    "# pprint(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasemos finalmente a implementar un algoritmo de evaluación de la secuencia de tokens a partir del parse izquierdo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mimetypes import init\n",
    "from typing import Tuple\n",
    "\n",
    "from matplotlib.artist import ArtistInspector\n",
    "\n",
    "\n",
    "def evaluate_parse(left_parse, tokens):\n",
    "    if not left_parse or not tokens:\n",
    "        return\n",
    "    \n",
    "    left_parse = iter(left_parse)\n",
    "    tokens = iter(tokens)\n",
    "    result = evaluate(next(left_parse), left_parse, tokens)\n",
    "    \n",
    "    assert isinstance(next(tokens).token_type, EOF)\n",
    "    return result\n",
    "    \n",
    "\n",
    "def evaluate(production, left_parse, tokens, inherited_value=None):\n",
    "    head, body = production\n",
    "    attributes = production.attributes\n",
    "    \n",
    "    # Insert your code here ...\n",
    "    # > synteticed = ...\n",
    "    # > inherited = ...\n",
    "    synteticed = []\n",
    "    inherited = []\n",
    "    for i in range(len(attributes)):\n",
    "        synteticed.append(None)\n",
    "        inherited.append(None)\n",
    "    \n",
    "    inherited[0] = inherited_value\n",
    "    for i, symbol in enumerate(body, 1):\n",
    "        if symbol.IsTerminal:\n",
    "            assert inherited[i] is None\n",
    "            if(body[i - 1] == num):\n",
    "                synteticed[i] = next(tokens).lex\n",
    "                inherited_value = attributes[0](inherited,synteticed)\n",
    "                return inherited_value\n",
    "            else:\n",
    "                synteticed[i] = next(tokens).lex\n",
    "                # attributes[i](inherited,synteticed)\n",
    "        else:\n",
    "            next_production = next(left_parse)\n",
    "            assert symbol == next_production.Left\n",
    "            if not attributes[i] == None:\n",
    "               synteticed[i] = evaluate(next_production, left_parse, tokens, attributes[i](inherited, synteticed))\n",
    "            else :\n",
    "                synteticed[i] = evaluate(next_production,left_parse,tokens, inherited_value)\n",
    "    # Insert your code here ...\n",
    "    return attributes[0](inherited, synteticed)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y enseguida podemos comprobar la correctitud del algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.9 + 4 = 9.9\n"
     ]
    }
   ],
   "source": [
    "result = evaluate_parse(left_parse, tokens)\n",
    "print(f'{text} = {result}')\n",
    "\n",
    "assert result == 9.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completando el pipeline\n",
    "\n",
    "Implementemos nuevamente un tokenizer muy básico. Asumiremos como de costumbre que las unidades léxicas relevantes están separadas por espacio (o sea, que los números y operadores están separados por al menos un espacio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_tokens = {\n",
    "    '+'  :   Token( '+', plus  ),\n",
    "    '-'  :   Token( '-', minus ),\n",
    "    '*'  :   Token( '*', star  ),\n",
    "    '/'  :   Token( '/', div   ),\n",
    "    '('  :   Token( '(', opar  ),\n",
    "    ')'  :   Token( ')', cpar  ),\n",
    "}\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "\n",
    "    for item in text.split():\n",
    "        try:\n",
    "            float(item)\n",
    "            token = Token(item, num)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                token = fixed_tokens[item]\n",
    "            except:\n",
    "                raise Exception('Undefined token')\n",
    "        tokens.append(token)\n",
    "\n",
    "    eof = Token('$', G.EOF)\n",
    "    tokens.append(eof)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos algunas cadenas. Se realizará la siguiente cadena de transformaciones:\n",
    "```\n",
    "Entrada -> Tokens -> Parse Izquierdo -> Resultado\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================Tokens================\n",
      "[\n",
      "   num: 1\n",
      "   -: -\n",
      "   num: 1\n",
      "   -: -\n",
      "   num: 1\n",
      "   $: $\n",
      "]\n",
      "==============Left-Parse==============\n",
      "[\n",
      "   E -> T X\n",
      "   T -> F Y\n",
      "   F -> num\n",
      "   Y -> e\n",
      "   X -> - T X\n",
      "   T -> F Y\n",
      "   F -> num\n",
      "   Y -> e\n",
      "   X -> - T X\n",
      "   T -> F Y\n",
      "   F -> num\n",
      "   Y -> e\n",
      "   X -> e\n",
      "]\n",
      "================Result================\n",
      "1 - 1 - 1 = -1.0\n"
     ]
    }
   ],
   "source": [
    "text = '1 - 1 - 1'\n",
    "tokens = tokenize_text(text)\n",
    "pprint(tokens, '================Tokens================')\n",
    "left_parse = parser(tokens)\n",
    "pprint(left_parse, '==============Left-Parse==============')\n",
    "result = evaluate_parse(left_parse, tokens)\n",
    "pprint(f'{text} = {result}', '================Result================')\n",
    "assert result == -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================Tokens================\n",
      "[\n",
      "   num: 1\n",
      "   -: -\n",
      "   (: (\n",
      "   num: 1\n",
      "   -: -\n",
      "   num: 1\n",
      "   ): )\n",
      "   $: $\n",
      "]\n",
      "==============Left-Parse==============\n",
      "[\n",
      "   E -> T X\n",
      "   T -> F Y\n",
      "   F -> num\n",
      "   Y -> e\n",
      "   X -> - T X\n",
      "   T -> F Y\n",
      "   F -> ( E )\n",
      "   E -> T X\n",
      "   T -> F Y\n",
      "   F -> num\n",
      "   Y -> e\n",
      "   X -> - T X\n",
      "   T -> F Y\n",
      "   F -> num\n",
      "   Y -> e\n",
      "   X -> e\n",
      "   Y -> e\n",
      "   X -> e\n",
      "]\n",
      "================Result================\n",
      "1 - ( 1 - 1 ) = 1.0\n"
     ]
    }
   ],
   "source": [
    "text = '1 - ( 1 - 1 )'\n",
    "tokens = tokenize_text(text)\n",
    "pprint(tokens, '================Tokens================')\n",
    "left_parse = parser(tokens)\n",
    "pprint(left_parse, '==============Left-Parse==============')\n",
    "result = evaluate_parse(left_parse, tokens)\n",
    "pprint(f'{text} = {result}', '================Result================')\n",
    "assert result == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propuestas\n",
    "\n",
    "- Con el objetivo de simplificar la implementación de los algoritmos en la clase, la evaluación de los atributos se realizó posteriormente a que se obtuviera completamente el parse izquierdo. Sin embargo, vimos en conferencia que la evaluación de los atributos puede realizarse junto al proceso de parsing LL(1) si la gramática es _L-atributada_. Realice las modificaciones pertinentes para evaluar los atributos a medida que se parsea la cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Iterator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [230], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTokensIterator\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, elements: Iterator[Token] ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melements \u001b[38;5;241m=\u001b[39m elements\n",
      "Cell \u001b[0;32mIn [230], line 2\u001b[0m, in \u001b[0;36mTokensIterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTokensIterator\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, elements: \u001b[43mIterator\u001b[49m[Token] ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melements \u001b[38;5;241m=\u001b[39m elements\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Iterator' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "class TokensIterator:\n",
    "    def __init__(self, elements: Iterator[Token] ) -> None:\n",
    "        self.elements = elements\n",
    "        self.current = None\n",
    "\n",
    "    def next(self):\n",
    "        self.current = next(self.elements)\n",
    "        return self.current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "   num: 5.9\n",
      "   +: +\n",
      "   num: 4\n",
      "   $: $\n",
      "]\n",
      "E  --->  [None]\n",
      "T  --->  [None]\n",
      "F  --->  [None]\n",
      "X  --->  [None, None, None]\n",
      "Y  --->  [None, None, None]\n",
      "+\n",
      "X\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Syntax error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [199], line 43\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(production, tokens, inherited_value)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m     P \u001b[38;5;241m=\u001b[39m \u001b[43mM\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_type\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: ('X', 'num')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [200], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(evaluate_witch_parser(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5.9 + 4\u001b[39m\u001b[38;5;124m'\u001b[39m,G))\n",
      "Cell \u001b[0;32mIn [199], line 10\u001b[0m, in \u001b[0;36mevaluate_witch_parser\u001b[0;34m(s, G)\u001b[0m\n\u001b[1;32m      8\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(tokens)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# token_current = next(tokens)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [199], line 45\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(production, tokens, inherited_value)\u001b[0m\n\u001b[1;32m     43\u001b[0m     P \u001b[38;5;241m=\u001b[39m M[top, s\u001b[38;5;241m.\u001b[39mtoken_type][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSyntax error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(P\u001b[38;5;241m.\u001b[39mRight \u001b[38;5;241m==\u001b[39m G\u001b[38;5;241m.\u001b[39mEpsilon):\n\u001b[1;32m     47\u001b[0m     valor \u001b[38;5;241m=\u001b[39m dicc[stack[\u001b[38;5;28mlen\u001b[39m(stack)\u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]][\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mException\u001b[0m: Syntax error"
     ]
    }
   ],
   "source": [
    "def metodo_predictivo_con_evaluacion(G: Grammar, M):\n",
    "\n",
    "    def parser(tokens: TokensIterator):\n",
    "        \n",
    "        stack: List[NonTerminal|Terminal] = [G.startSymbol]\n",
    "        cursor = 0\n",
    "        \n",
    "        while True:\n",
    "            top = stack.pop()\n",
    "            token = tokens.current\n",
    "\n",
    "            if top.IsNonTerminal:\n",
    "                try:\n",
    "                    print(top, token.token_type)\n",
    "                    production: AttributeProduction = M[top, token.token_type][0]\n",
    "                except KeyError:\n",
    "                    raise Exception(\"No se puede reconocer la cadena\")\n",
    "                \n",
    "                yield production\n",
    "\n",
    "                for symbol in reversed(production.Right):\n",
    "                    stack.append(symbol)\n",
    "            \n",
    "            if len(stack)==0:\n",
    "                break\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [ Token('5.9', num), Token('+', plus), Token('4', num), Token('$', G.EOF) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lazy_evaluate_parse(G, M, tokens):\n",
    "    parser = metodo_predictivo_con_evaluacion(G,M)\n",
    "    tokens_iterator = TokensIterator(iter(tokens))\n",
    "    tokens_iterator.next()\n",
    "    left_parse = parser(tokens_iterator)\n",
    "\n",
    "    if not parser or not tokens:\n",
    "        return\n",
    "\n",
    "    final_value = None\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            result = next(lazy_evaluate(next(left_parse), left_parse, tokens_iterator))\n",
    "            final_value = result\n",
    "        except StopIteration:\n",
    "            break\n",
    "    \n",
    "    assert isinstance(tokens_iterator.current.token_type, EOF)\n",
    "    \n",
    "    return final_value\n",
    "    \n",
    "\n",
    "def lazy_evaluate(production: AttributeProduction, left_parse, tokens: TokensIterator, inherited_value=None):\n",
    "    head, body = production\n",
    "    attributes = production.attributes\n",
    "\n",
    "    synteticed = [None for x in attributes]\n",
    "    inherited = [None for x in attributes]\n",
    "    \n",
    "    if not inherited_value is None:\n",
    "        inherited[0] = inherited_value\n",
    "    \n",
    "    for i, symbol in enumerate(body, 1):\n",
    "        if symbol.IsTerminal:\n",
    "            assert inherited[i] is None\n",
    "            token = tokens.current\n",
    "            \n",
    "            if token.token_type == num:\n",
    "                synteticed[i] = float(token.lex)\n",
    "            else: \n",
    "                synteticed[i] = token.lex\n",
    "            \n",
    "            tokens.next()\n",
    "        else:\n",
    "            \n",
    "            next_production = next(left_parse)\n",
    "            # print(production)\n",
    "            # print('symbol', symbol)\n",
    "            # print('next', next_production)\n",
    "            assert symbol == next_production.Left\n",
    "\n",
    "            if not attributes[i]:\n",
    "                synteticed[i] = next(lazy_evaluate(next_production, left_parse, tokens))\n",
    "            else:\n",
    "                attr_value = attributes[i](inherited, synteticed)\n",
    "                synteticed[i] = next(lazy_evaluate(next_production, left_parse, tokens, attr_value))\n",
    "    \n",
    "    yield attributes[0](inherited, synteticed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '5.9 + 4'\n",
    "result = lazy_evaluate_parse(G, M, tokens)\n",
    "print(f'{text} = {result}')\n",
    "assert result == 9.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '1 - 1 - 1'\n",
    "tokens = tokenize_text(text)\n",
    "result = lazy_evaluate_parse(G,M, tokens)\n",
    "pprint(f'{text} = {result}', '================Result================')\n",
    "assert result == -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '1 - ( 1 - 1 )'\n",
    "tokens = tokenize_text(text)\n",
    "result = lazy_evaluate_parse(G, M, tokens)\n",
    "pprint(f'{text} = {result}', '================Result================')\n",
    "assert result == 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
