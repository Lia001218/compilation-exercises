{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Práctica # 8 (Compilación)\n",
    "\n",
    "En esta clase estaremos implementando un **intérprete de expresiones regulares**. Utilizaremos autómatas finitos como mecanismo reconocedor del lenguaje que denota cada expresión regular. Nos apoyaremos en las operaciones entre autómatas implementadas en las clases anteriores para ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.tools.automata import NFA, DFA, nfa_to_dfa\n",
    "from cmp.tools.automata import automata_union, automata_concatenation, automata_closure, automata_minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expresiones regulares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluar una expresión de determinado dominio no debería parecernos un problema salido de la nada. Desde comienzos del curso, y hasta unas clases atrás, estuvimos enfrentándonos al problema de evaluar expresiones aritméticas. Ahora nos enfrentamos a un problema similar, cambiando sumas por uniones, productos por concatenaciones, etc. Esto implica que la metodología que usaremos es la misma: obtendremos una **representación semántica**, **parseando** según una **gramática** del lenguaje de expresiones, cuyos símbolos son los **tokens** que obtenemos del **lexer**.\n",
    "\n",
    "Curiosamente, llegamos este punto con el objetivo de implementar el lexer que alimente al parser durante la contrucción del compilador. Ahora nos apoyaremos en todo lo implementado del parser para construir el lexer. Claramente, el lexer (_tokenizer_) que usaremos para construir el generador de lexer será un versión básica, a _pico y pala_, pues los tokens de las expresiones regulares son muy fáciles de extraer.\n",
    "\n",
    "### Nodos del AST\n",
    "\n",
    "Pasemos a definir los nodos del AST. Usaremos como base las clases `Node`, `AtomicNode`, `UnaryNode` y `BinaryNode` para mantener la compatibilidad con el `printer` de AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def evaluate(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "class AtomicNode(Node):\n",
    "    def __init__(self, lex):\n",
    "        self.lex = lex\n",
    "\n",
    "class UnaryNode(Node):\n",
    "    def __init__(self, node):\n",
    "        self.node = node\n",
    "        \n",
    "    def evaluate(self):\n",
    "        value = self.node.evaluate() \n",
    "        return self.operate(value)\n",
    "    \n",
    "    @staticmethod\n",
    "    def operate(value):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "class BinaryNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        \n",
    "    def evaluate(self):\n",
    "        lvalue = self.left.evaluate() \n",
    "        rvalue = self.right.evaluate()\n",
    "        return self.operate(lvalue, rvalue)\n",
    "    \n",
    "    @staticmethod\n",
    "    def operate(lvalue, rvalue):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "from cmp.ast import get_printer\n",
    "printer = get_printer(AtomicNode=AtomicNode, UnaryNode=UnaryNode, BinaryNode=BinaryNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `evaluate` debe compilar la expresión regular. Dicho de otra forma, debe devolver el `NFA` que reconoce el lenguaje denotado por la expresión regular en cuestión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 'ε'\n",
    "\n",
    "class EpsilonNode(AtomicNode):\n",
    "    def evaluate(self):\n",
    "        # Your code here!!!\n",
    "        pass\n",
    "\n",
    "EpsilonNode(EPSILON).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymbolNode(AtomicNode):\n",
    "    def evaluate(self):\n",
    "        s = self.lex\n",
    "        # Your code here!!!\n",
    "\n",
    "SymbolNode('a').evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClosureNode(UnaryNode):\n",
    "    @staticmethod\n",
    "    def operate(value):\n",
    "        # Your code here!!!\n",
    "        pass\n",
    "    \n",
    "ClosureNode(SymbolNode('a')).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnionNode(BinaryNode):\n",
    "    @staticmethod\n",
    "    def operate(lvalue, rvalue):\n",
    "        # Your code here!!!\n",
    "        pass\n",
    "\n",
    "UnionNode(SymbolNode('a'), SymbolNode('b')).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatNode(BinaryNode):\n",
    "    @staticmethod\n",
    "    def operate(lvalue, rvalue):\n",
    "        # Your code here!!!\n",
    "        pass\n",
    "\n",
    "ConcatNode(SymbolNode('a'), SymbolNode('b')).evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gramática\n",
    "\n",
    "Habiendo definido los nodos del AST, pasemos a diseñar la gramática atributada para construirlo. Recordemos que es necesario que la gramática no sea ambigua (para ser parseable), no tener prefijos comunes ni recursividad izquierda (para ser parseable con un parser LL(1)) y que respete la prioridad de los operadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Terminals:\n",
      "\tE, T, F, A, X, Y, Z\n",
      "Terminals:\n",
      "\t|, *, (, ), symbol, ε\n",
      "Productions:\n",
      "\t[]\n"
     ]
    }
   ],
   "source": [
    "from cmp.pycompiler import Grammar\n",
    "\n",
    "G = Grammar()\n",
    "\n",
    "E = G.NonTerminal('E', True)\n",
    "T, F, A, X, Y, Z = G.NonTerminals('T F A X Y Z')\n",
    "pipe, star, opar, cpar, symbol, epsilon = G.Terminals('| * ( ) symbol ε')\n",
    "\n",
    "# > PRODUCTIONS???\n",
    "# Your code here!!!\n",
    "\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Para el lexer a _pico y pala_, procederemos como de costumbre: mantendremos una colección con los tokens de lexema fijo y cualquier otro elemento será tratado como símbolo. Los lexemas no se obtendrán de separar por espacios, sino de considerar cada caracter de la cadena original como lexema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[$: $]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cmp.utils import Token\n",
    "\n",
    "def regex_tokenizer(text, G, skip_whitespaces=True):\n",
    "    tokens = []\n",
    "    # > fixed_tokens = ???\n",
    "    # Your code here!!!\n",
    "    for char in text:\n",
    "        if skip_whitespaces and char.isspace():\n",
    "            continue\n",
    "        # Your code here!!!\n",
    "        \n",
    "    tokens.append(Token('$', G.EOF))\n",
    "    return tokens\n",
    "\n",
    "tokens = regex_tokenizer('a*(a|b)*cd | ε',G)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser\n",
    "\n",
    "Usaremos un parser LL(1) para obtener un parse izquierdo de la cadena (expresión regular)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error. Aborting...\n"
     ]
    }
   ],
   "source": [
    "from cmp.tools.parsing import metodo_predictivo_no_recursivo\n",
    "\n",
    "parser = metodo_predictivo_no_recursivo(G)\n",
    "left_parse = parser(tokens)\n",
    "left_parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AST\n",
    "\n",
    "Para obtener el AST evaluaremos los atributos de la gramática. Esto devolverá el AST sintetizado en la producción raíz de la gramática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from cmp.tools.evaluation import evaluate_parse\n",
    "\n",
    "ast = evaluate_parse(left_parse, tokens)\n",
    "print(printer(ast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autómata\n",
    "\n",
    "Y para obtener el autómata simplemente invocamos el método `evaluate` de la raíz del AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nfa \u001b[38;5;241m=\u001b[39m \u001b[43mast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m()\n\u001b[1;32m      2\u001b[0m nfa\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "nfa = ast.evaluate()\n",
    "nfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convirtámoslo ahora en DFA para comprobar que reconoce las cadenas del lenguaje denotado por la expresión regular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = nfa_to_dfa(nfa)\n",
    "display(dfa)\n",
    "\n",
    "assert dfa.recognize('')\n",
    "assert dfa.recognize('cd')\n",
    "assert dfa.recognize('aaaaacd')\n",
    "assert dfa.recognize('bbbbbcd')\n",
    "assert dfa.recognize('bbabababcd')\n",
    "assert dfa.recognize('aaabbabababcd')\n",
    "\n",
    "assert not dfa.recognize('cda')\n",
    "assert not dfa.recognize('aaaaa')\n",
    "assert not dfa.recognize('bbbbb')\n",
    "assert not dfa.recognize('ababba')\n",
    "assert not dfa.recognize('cdbaba')\n",
    "assert not dfa.recognize('cababad')\n",
    "assert not dfa.recognize('bababacc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, aplicaremos el algoritmo de minimización de autómatas para obtener una versión más compacta del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = automata_minimization(dfa)\n",
    "display(mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propuestas\n",
    "\n",
    "- Implemente un intérprete para _expresiones regulares extendidas_ (operadores: `+`, `?`, `[ ]`, etc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
