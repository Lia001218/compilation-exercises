{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Práctica #1 (Compilación)\n",
    "\n",
    "A lo largo del curso estaremos implementando un compilador para el lenguaje de programación HULK, paso a paso, introduciendo nuevas características del lenguaje o mejorando la implementación de otras características a medida que vamos descubriendo las técnicas fundamentales de la teoría de lenguajes y la compilación.\n",
    "\n",
    "El objetivo de esta clase es construir un evaluador de expresiones \"a mano\", usando los recursos que tenemos hasta el momento. Para ello vamos a comenzar con una versión de HULK muy sencilla, un lenguaje de expresiones aritméticas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluador de expresiones\n",
    "\n",
    "Definiremos a continuación este lenguaje de manera informal:\n",
    "\n",
    "Un programa en `HULK` (la `x` por `expression`) consta de una secuencia de expresiones. Cada expresión está compuesta por:\n",
    "\n",
    "- números (con coma flotante de 32 bits), \n",
    "- operadores `+ - * /` con el orden operacional, y\n",
    "- paréntesis `(` y `)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis lexicográfico\n",
    "\n",
    "Comenzaremos construyendo un prototipo bien simple, donde asumiremos que en la expresión hay espacios en blanco entre todos los elementos, de modo que el *lexer* se reduce a dividir por espacios. Luego iremos adicionando elementos más complejos.\n",
    "\n",
    "El siguiente método devuelve una lista de *tokens*, asumiendo que la expresión solo tiene números, operadores y paréntesis, separados por espacios en blanco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Returns the set of tokens. At this point, simply splits by \n",
    "    spaces and converts numbers to `float` instances.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for item in text.split():\n",
    "        if item.isnumeric():\n",
    "            tokens.append(int(item))  \n",
    "        else :\n",
    "            tokens.append(item)\n",
    "    return tokens\n",
    "\n",
    "assert tokenize('5 + 6 * 9') == [5, '+', 6, '*', 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis sintáctico y evaluación\n",
    "\n",
    "Una vez que tenemos los *tokens*, solo nos queda evaluar la expresión. Usaremos para ello una idea simple, pero bien útil: evaluaremos recursivamente la expresión descendiendo por los distintos niveles de precedencia.\n",
    "\n",
    "Toda expresión del lenguaje puede ser vista como una suma o resta de _términos_, donde cada uno de estos \"_términos_\" se descompone a su vez en operaciones (`*` y `/`) entre _factores_. Incluso si no hay operadores `+` y `-` en toda la expresión queda claro que esta idea es válida puesto que estaríamos en presencia de una expresión formada por un solo _término_. Los _factores_ del lenguaje son todos unidades atómicas: por ahora solo números y expresiones complejas envueltas entre paréntesis. Nótese que el uso de paréntesis permite reiniciar el descenso por los niveles de precedencia (regresar a los niveles más altos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lambda expressions map from operators to actual executable code\n",
    "operations = {\n",
    "    '+': lambda x,y: x + y,\n",
    "    '-': lambda x,y: x - y,\n",
    "    '*': lambda x,y: x * y,\n",
    "    '/': lambda x,y: x / y,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some util classes and methods\n",
    "\n",
    "class ParsingError(Exception):\n",
    "    \"\"\"\n",
    "    Base class for all parsing exceptions.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class BadEOFError(ParsingError):\n",
    "    \"\"\"\n",
    "    Unexpected EOF error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        ParsingError.__init__(self, \"Unexpected EOF\")\n",
    "        \n",
    "class UnexpectedToken(ParsingError):\n",
    "    \"\"\"\n",
    "    Unexpected token error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token, i):\n",
    "        ParsingError.__init__(self, f'Unexpected token: {token} at {i}')\n",
    "        \n",
    "class MissingCloseParenthesisError(ParsingError):\n",
    "    \"\"\"\n",
    "    Missing ')' token error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token, i):\n",
    "        ParsingError.__init__(self, f'Expected \")\" token at {i}. Got \"{token}\" instead')\n",
    "        \n",
    "class MissingOpenParenthesisError(ParsingError):\n",
    "    \"\"\"\n",
    "    Missing '(' token error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token, i):\n",
    "        ParsingError.__init__(self, f'Expected \"(\" token at {i}. Got \"{token}\" instead')\n",
    "\n",
    "def get_token(tokens, i, error_type=BadEOFError):\n",
    "    \"\"\"\n",
    "    Returns tokens[i] if 'i' is in range. Otherwise, raises ParsingError exception.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return tokens[i]\n",
    "    except IndexError:\n",
    "        raise error_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "from configparser import MissingSectionHeaderError\n",
    "from operator import index\n",
    "\n",
    "\n",
    "def evaluate(tokens):\n",
    "    \"\"\"\n",
    "    Evaluates an expression recursively.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        i, value = parse_expression(tokens, 0)\n",
    "        assert i == len(tokens)\n",
    "        return value\n",
    "    except ParsingError as error:\n",
    "        print(error)\n",
    "        return None\n",
    "\n",
    "def parse_expression(tokens, i):\n",
    "    i, term = parse_term(tokens, i)\n",
    "    return parse_group_of_terms(tokens, i, term)\n",
    "    \n",
    "def parse_group_of_terms(tokens, i, value):\n",
    "    if i < len(tokens):\n",
    "        if tokens[i] == '+' or tokens[i] == '-':\n",
    "            lambda_ = operations[tokens[i]]\n",
    "            if(i == len(tokens) - 1):\n",
    "                raise Exception('Un operador no puede ser el final de la cadena')\n",
    "            else:\n",
    "                i,value1 = parse_term(tokens,i+1)\n",
    "                i,value2 = parse_group_of_terms(tokens,i,value1)\n",
    "                return i , lambda_(value2,value)\n",
    "    return i, value\n",
    "        \n",
    "def parse_term(tokens, i):\n",
    "    i, value  = parse_factor(tokens,i)\n",
    "    i, value = parse_group_of_factors(tokens,i,value)\n",
    "    \n",
    "    return i , value\n",
    "\n",
    "\n",
    "def parse_group_of_factors(tokens, i, value):\n",
    "    if i < len(tokens):\n",
    "        if tokens[i] == '*' or tokens[i] == '/':\n",
    "            lambda_ = operations[tokens[i]]\n",
    "            if(i == len(tokens) - 1):\n",
    "                raise Exception('Un operador no puede ser el final de la cadena')\n",
    "            else:\n",
    "                i,value1 = parse_factor(tokens,i+1)\n",
    "                i,value2 = parse_group_of_factors(tokens,i,lambda_(value,value1))\n",
    "                return i , value2\n",
    "    return i , value\n",
    "\n",
    "def parse_factor(tokens, i):\n",
    "   if i < len(tokens):\n",
    "    index1 = get_token(tokens,i)\n",
    "    # if(tokens[i] in operations and i == len(tokens) - 1):\n",
    "    #     print('entre')\n",
    "    #     raise Exception('pasaron cosas')\n",
    "    # elif(index1 is not operations and index1.isnumeric() == False ):\n",
    "    #     raise MissingSectionHeaderError()\n",
    "    if index1 == '(':\n",
    "        i, exp = parse_expression(tokens,i+1)\n",
    "        index2 = get_token(tokens , i)\n",
    "        if tokens[i] != ')' :\n",
    "            raise MissingCloseParenthesisError()\n",
    "        return i+1 , exp\n",
    "    else :\n",
    "        return i+1,tokens[i]\n",
    "\n",
    "\n",
    "            \n",
    "assert evaluate(tokenize('5 + 6 * 9')) == 59.\n",
    "assert evaluate(tokenize('( 5 + 6 ) * 9')) == 99.\n",
    "assert evaluate(tokenize('( 5 + 6 ) + 1 * 9 + 2')) == 22.\n",
    "assert evaluate(tokenize('1 - 1 + 1')) == 1\n",
    "assert evaluate(tokenize('8 / 4 / 2')) == 1\n",
    "print(evaluate(tokenize('2 *  3 * 4 ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando constantes\n",
    "\n",
    "Agreguemos constantes numéricas al lenguaje `HULK` Para ello, simplemente añadiremos un diccionario con todas las constantes disponibles, que usaremos durante la tokenización. Nótese que solo es necesario modificar el _lexer_ para añadir este rasgo al lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = {\n",
    "    'pi': 3.14159265359,\n",
    "    'e': 2.71828182846,\n",
    "    'phi': 1.61803398875,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize import Double\n",
    "\n",
    "\n",
    "def tokenize(expr):\n",
    "    \"\"\"\n",
    "    Returns the set of tokens. At this point, simply splits by \n",
    "    spaces and converts numbers to `float` instances.\n",
    "    Replaces constants.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    \n",
    "    for token in expr.split():\n",
    "        if token == 'pi' or token == 'e' or token == 'phi' :\n",
    "            tokens.append(constants[token])\n",
    "        elif token.isnumeric() :\n",
    "            tokens.append(float(token))\n",
    "        else:\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "assert tokenize('2 * pi') == [2.0, '*', 3.14159265359]\n",
    "assert evaluate(tokenize('2 * pi')) == 6.28318530718"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando funciones elementales\n",
    "\n",
    "Agreguemos funciones elementales `sin`, `cos`, `tan`, `log`, `sqrt`, etc. El llamado a funciones se hará en notación prefija, comenzando por el nombre de la función y seguido, entre paréntesis, por los argumentos, que estarán separados entre sí por _comas_.\n",
    "\n",
    "Para las funciones elementales haremos algo similar a las constantes, pero en vez de a la hora de tokenizar, las reemplazaremos a la hora de evaluar, pues necesitamos evaluar recursivamente los argumentos de la función. Empezaremos por garantizar que nuestro tokenizador que es capaz de reconocer expresiones con funciones elementales de más de un argumento, en caso de no ser así es necesario arreglarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenize('log ( 64 , 1 + 3 )') == ['log', '(', 64.0, ',', 1.0, '+', 3.0, ')']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionaremos entonces un diccionario con todas las funciones elementales que permitiremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "functions = {\n",
    "    'sin': lambda x: math.sin(x),\n",
    "    'cos': lambda x: math.cos(x),\n",
    "    'tan': lambda x: math.tan(x),\n",
    "    'log': lambda x,y: math.log(x, y),\n",
    "    'sqrt': lambda x: math.sqrt(x),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, modificaremos el método `evaluate` para que use las funciones elementales. Recordemos que los argumentos están separados por el token _coma_ (`,`) y que cada uno puede a su vez tener sub-expresiones que consistan también en llamados a funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Value\n",
    "from tabnanny import check\n",
    "\n",
    "\n",
    "def parse_factor(tokens, i):\n",
    "    # Insert your code here ...\n",
    "    if i < len(tokens):\n",
    "        index1 = get_token(tokens,i)\n",
    "        if index1 == '(':\n",
    "            i, exp = parse_expression(tokens,i+1)\n",
    "            index2 = get_token(tokens , i)\n",
    "            if index2!= ')' :\n",
    "                raise MissingCloseParenthesisError()\n",
    "            return i+1 , exp\n",
    "        elif index1 in functions :\n",
    "            func_lambda = functions[index1]\n",
    "            check_oper_open = get_token(tokens, i+1)\n",
    "            if(check_oper_open == '('):\n",
    "                i, exp1 = parse_expression(tokens,i+2)\n",
    "                if (index1 == 'log'):\n",
    "                    if(tokens[i]== ','):\n",
    "                        i,exp2 = parse_expression(tokens,i+1)\n",
    "                        value = func_lambda(exp1,exp2)\n",
    "                    else :\n",
    "                        raise Exception('Faltan argumentos al log')\n",
    "                else : value = func_lambda(exp1)\n",
    "                if(get_token(tokens,i) == ')'):\n",
    "                    return i+1, value\n",
    "                else : raise MissingCloseParenthesisError()\n",
    "            else : raise MissingOpenParenthesisError()\n",
    "        else :\n",
    "            return i+1,tokens[i]\n",
    "    \n",
    "assert evaluate(tokenize('log ( 64 , 1 + 3 )')) == 3.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
